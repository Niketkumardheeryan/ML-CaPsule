{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNN) Documentation\n",
    "\n",
    "## Introduction\n",
    "A Convolutional Neural Network (CNN) is a deep learning algorithm that can take in an input image, assign importance to various aspects/objects in the image, and be able to differentiate one from the other. This notebook will guide you through the creation, functioning, and hyperparameters of a CNN using Python and popular libraries such as TensorFlow and Keras. We will also provide examples to demonstrate how to build, train, and evaluate a CNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Installation (if not already installed)\n",
    "# !pip install numpy tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and Preprocessing \n",
    "loading the MNIST dataset, which is a widely-used dataset of handwritten digits. Here's a brief explanation of each step:\n",
    "\n",
    "**Load Dataset**: The mnist.load_data() function loads the MNIST dataset, returning two tuples: (X_train, y_train) for training data and labels, and (X_test, y_test) for test data and labels.\n",
    "\n",
    "**Reshape Data**: The MNIST dataset originally consists of 28x28 pixel grayscale images. The reshape() function is used to reshape the data to fit the model. Here, we add an extra dimension to the data to indicate the single channel (grayscale) since convolutional layers expect input in the shape (height, width, channels).\n",
    "\n",
    "**Normalize Data**: The pixel values of the images are scaled to the range [0, 1] by dividing by 255. This step is essential to ensure that the input values fall within a similar range, which helps the optimization algorithm converge faster.\n",
    "\n",
    "**One-Hot Encode Labels**: The to_categorical() function is used to convert the class labels into one-hot encoded format. In the MNIST dataset, there are 10 classes (digits 0 through 9), so each label is represented as a binary vector of length 10, with a 1 at the index corresponding to the class and 0s elsewhere.\n",
    "\n",
    "**Display Dataset Information**: Finally, the shapes of the training and test datasets are printed to verify that the preprocessing steps were successful. The training data shape indicates the number of samples, height, width, and number of channels, while the test data shape provides similar information for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28, 1)\n",
      "Test data shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape data to fit the model\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Display dataset information\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Conv2D layer with 32 filters and 3x3 kernel size.\n",
      "Added MaxPooling layer with 2x2 pool size.\n",
      "Added second Conv2D layer with 64 filters and 3x3 kernel size.\n",
      "Added second MaxPooling layer with 2x2 pool size.\n",
      "Added Flatten layer.\n",
      "Added Dense layer with 128 neurons.\n",
      "Added Dropout layer with 0.5 rate.\n",
      "Added output layer with 10 neurons and softmax activation function.\n",
      "Compiled the model with Adam optimizer and categorical crossentropy loss function.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               204928    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 225,034\n",
      "Trainable params: 225,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layer with 32 filters, kernel size of 3x3, ReLU activation function\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "print(\"Added Conv2D layer with 32 filters and 3x3 kernel size.\")\n",
    "\n",
    "# MaxPooling layer with pool size of 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "print(\"Added MaxPooling layer with 2x2 pool size.\")\n",
    "\n",
    "# Second Convolutional layer with 64 filters, kernel size of 3x3, ReLU activation function\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "print(\"Added second Conv2D layer with 64 filters and 3x3 kernel size.\")\n",
    "\n",
    "# Second MaxPooling layer with pool size of 2x2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "print(\"Added second MaxPooling layer with 2x2 pool size.\")\n",
    "\n",
    "# Flatten the data for fully connected layers\n",
    "model.add(Flatten())\n",
    "print(\"Added Flatten layer.\")\n",
    "\n",
    "# Fully connected layer with 128 neurons, ReLU activation function\n",
    "model.add(Dense(128, activation='relu'))\n",
    "print(\"Added Dense layer with 128 neurons.\")\n",
    "\n",
    "# Dropout layer to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "print(\"Added Dropout layer with 0.5 rate.\")\n",
    "\n",
    "# Output layer with 10 neurons (one for each class), softmax activation function\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "print(\"Added output layer with 10 neurons and softmax activation function.\")\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical crossentropy loss function\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"Compiled the model with Adam optimizer and categorical crossentropy loss function.\")\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Hyperparameters are the parameters that are not learned during training but are set before the training process. Some important hyperparameters in CNNs include:\n",
    "\n",
    "- **Learning Rate**: Controls how much to change the model in response to the estimated error each time the model weights are updated.\n",
    "- **Batch Size**: The number of samples processed before the model is updated.\n",
    "- **Epochs**: The number of times the entire dataset is passed forward and backward through the neural network.\n",
    "- **Kernel Size**: The size of the filter in the convolutional layers.\n",
    "- **Number of Filters**: The number of filters in the convolutional layers.\n",
    "- **Dropout Rate**: The fraction of neurons to drop during training to prevent overfitting.\n",
    "\n",
    "In this example, we use:\n",
    "- Optimizer: Adam\n",
    "- Loss function: Categorical Crossentropy\n",
    "- Metrics: Accuracy\n",
    "- Batch size: 128\n",
    "- Epochs: 12\n",
    "- Kernel size: 3x3\n",
    "- Number of filters: 32 and 64\n",
    "- Dropout rate: 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "375/375 [==============================] - 26s 67ms/step - loss: 0.3430 - accuracy: 0.8950 - val_loss: 0.0829 - val_accuracy: 0.9759\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 23s 60ms/step - loss: 0.1158 - accuracy: 0.9656 - val_loss: 0.0632 - val_accuracy: 0.9808\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 22s 59ms/step - loss: 0.0845 - accuracy: 0.9754 - val_loss: 0.0548 - val_accuracy: 0.9831\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 27s 72ms/step - loss: 0.0686 - accuracy: 0.9791 - val_loss: 0.0498 - val_accuracy: 0.9855\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.0581 - accuracy: 0.9826 - val_loss: 0.0410 - val_accuracy: 0.9885\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Visualize training process\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example predictions\n",
    "predictions = model.predict(X_test[:5])\n",
    "print(\"Predicted probabilities:\\n\", predictions)\n",
    "print(\"Predicted classes:\\n\", np.argmax(predictions, axis=1))\n",
    "print(\"Actual classes:\\n\", np.argmax(y_test[:5], axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we have built and trained a Convolutional Neural Network using TensorFlow and Keras. We covered the preprocessing of data, building the model, setting hyperparameters, training, and evaluating the model.\n",
    "\n",
    "### Further Reading\n",
    "- [Deep Learning with Python by François Chollet](https://www.manning.com/books/deep-learning-with-python)\n",
    "- [TensorFlow Documentation](https://www.tensorflow.org/learn)\n",
    "- [Keras Documentation](https://keras.io/)\n",
    "\n",
    "### Improvements\n",
    "- Experiment with different architectures and hyperparameters.\n",
    "- Use cross-validation for more robust model evaluation.\n",
    "- Apply this approach to other image datasets and problem domains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
