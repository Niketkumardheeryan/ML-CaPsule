{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs) are a type of neural network particularly well-suited for sequence data. They have applications in natural language processing, time series analysis, and sequence prediction tasks.RNNs are designed to retain information over time by utilizing recurrent connections. Each neuron in an RNN receives input not only from the current time step but also from the previous time step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Loading and preprocessing data\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load IMDb dataset\n",
    "max_words = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)\n",
    "\n",
    "# Preprocess data\n",
    "max_len = 100\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the RNN model\n",
    "**Sequential Model**: The Sequential model allows you to create neural networks layer by layer in a linear stack. In this example, we're building a model for binary classification.\n",
    "\n",
    "**Embedding Layer**: The Embedding layer is responsible for converting input sequences (represented as integers) into dense vectors of fixed size. It turns positive integers (indexes) into dense vectors of fixed size. In this case, it transforms words represented by integers into 32-dimensional dense vectors.\n",
    "\n",
    "**SimpleRNN Layer**: The SimpleRNN layer is the recurrent layer of the model. It processes sequences by iterating through the input sequence elements and maintaining a state. This layer has 32 units, which means it will output a 32-dimensional vector at each time step.\n",
    "\n",
    "**Dense Layer**: The Dense layer is a fully connected layer that performs classification. In this case, it has 1 unit with a sigmoid activation function, suitable for binary classification problems.\n",
    "\n",
    "**Model Compilation**: Finally, the model is compiled with the Adam optimizer, binary crossentropy loss function (suitable for binary classification), and accuracy as the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Building an RNN model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=32, input_length=max_len))\n",
    "model.add(SimpleRNN(units=32))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Explanation:\n",
    "\n",
    "- **Embedding Layer**:\n",
    "  - `input_dim`: The size of the vocabulary, i.e., the number of unique words in the corpus.\n",
    "  - `output_dim`: The dimension of the dense embedding vectors. It represents the size of the vector space in which words will be embedded.\n",
    "  - `input_length`: The length of input sequences, which should be consistent across all sequences. In this case, it's set to `max_len`.\n",
    "\n",
    "- **SimpleRNN Layer**:\n",
    "  - `units`: The dimensionality of the output space (i.e., the number of units or cells) in the RNN layer. Higher values allow the model to capture more complex patterns but also increase computational complexity.\n",
    "\n",
    "- **Dense Layer**:\n",
    "  - `units`: The dimensionality of the output space of the Dense layer. In this case, it's set to 1 for binary classification.\n",
    "  - `activation`: The activation function applied to the output of the Dense layer. Here, 'sigmoid' is used for binary classification problems to output probabilities between 0 and 1.\n",
    "\n",
    "- **Model Compilation**:\n",
    "  - `optimizer`: The optimizer algorithm used to minimize the loss function. 'adam' is an adaptive learning rate optimization algorithm that's widely used in deep learning.\n",
    "  - `loss`: The loss function used to compute the error of the model during training. 'binary_crossentropy' is commonly used for binary classification problems.\n",
    "  - `metrics`: A list of metrics used to evaluate the performance of the model during training and testing. 'accuracy' measures the accuracy of binary classification predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 6s 29ms/step - loss: 0.6637 - accuracy: 0.5885 - val_loss: 0.6179 - val_accuracy: 0.6700\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 5s 29ms/step - loss: 0.4037 - accuracy: 0.8257 - val_loss: 0.4043 - val_accuracy: 0.8188\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 4s 29ms/step - loss: 0.2265 - accuracy: 0.9136 - val_loss: 0.4113 - val_accuracy: 0.8236\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 5s 31ms/step - loss: 0.1070 - accuracy: 0.9677 - val_loss: 0.4783 - val_accuracy: 0.8296\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 5s 32ms/step - loss: 0.0414 - accuracy: 0.9912 - val_loss: 0.5719 - val_accuracy: 0.7958\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 5s 29ms/step - loss: 0.0203 - accuracy: 0.9966 - val_loss: 0.6334 - val_accuracy: 0.7920\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.0095 - accuracy: 0.9991 - val_loss: 0.6639 - val_accuracy: 0.8054\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 4s 28ms/step - loss: 0.0047 - accuracy: 0.9998 - val_loss: 0.7072 - val_accuracy: 0.8166\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 5s 32ms/step - loss: 0.0031 - accuracy: 0.9999 - val_loss: 0.7541 - val_accuracy: 0.8232\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 5s 33ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7725 - val_accuracy: 0.8170\n"
     ]
    }
   ],
   "source": [
    "# Example: Training the RNN model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 8ms/step - loss: 0.7814 - accuracy: 0.8142\n",
      "Test Loss: 0.7814052104949951, Test Accuracy: 0.8142399787902832\n"
     ]
    }
   ],
   "source": [
    "# Example: Evaluating the trained model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 320ms/step\n"
     ]
    }
   ],
   "source": [
    "# Example: Using the trained RNN model for sequence prediction\n",
    "predictions = model.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Conclusion\n",
    "In this documentation, we've explored the fundamentals of Recurrent Neural Networks (RNNs) and demonstrated how to build, train, and evaluate an RNN model using TensorFlow and Keras. RNNs have proven to be powerful tools for sequence modeling tasks, including natural language processing, time series analysis, and sequential data prediction.\n",
    "\n",
    "We've discussed the architecture of RNNs, which allows them to retain memory across sequential data, making them suitable for tasks where context and temporal dependencies are essential. Through code examples, we've shown how to construct an RNN model using layers such as Embedding, SimpleRNN, and Dense, and how to compile the model with appropriate loss functions, optimizers, and evaluation metrics.\n",
    "\n",
    "Understanding the hyperparameters of RNNs, such as the number of units, input length, and optimizer algorithms, is crucial for optimizing model performance. By experimenting with different hyperparameter configurations, researchers and practitioners can fine-tune RNN models to achieve better accuracy and generalization on specific tasks.\n",
    "\n",
    "In conclusion, RNNs offer a flexible framework for modeling sequential data and have become indispensable in various domains. We encourage further exploration and experimentation with RNN architectures and techniques to harness their full potential in solving real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
