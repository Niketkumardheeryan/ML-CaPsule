{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSXVGbL2kJgF",
        "outputId": "b094f8fe-dcb4-4aac-c333-8c8905003061"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data:\n",
            "                 id                                       comment_text  toxic  \\\n",
            "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
            "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
            "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
            "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
            "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
            "\n",
            "   severe_toxic  obscene  threat  insult  identity_hate  \n",
            "0             0        0       0       0              0  \n",
            "1             0        0       0       0              0  \n",
            "2             0        0       0       0              0  \n",
            "3             0        0       0       0              0  \n",
            "4             0        0       0       0              0  \n",
            "\n",
            "Test Data:\n",
            "                 id                                       comment_text\n",
            "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
            "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
            "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
            "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
            "4  00017695ad8997eb          I don't anonymously edit articles at all.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# Define file paths (update these paths based on your actual file locations)\n",
        "train_path = 'Dataset/train.csv'\n",
        "test_path = 'Dataset/test.csv'\n",
        "\n",
        "# Load CSV files into Pandas DataFrames\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "# Display the first few rows of each dataset\n",
        "print(\"Train Data:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\nTest Data:\")\n",
        "print(test_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAuOW-vuozmP",
        "outputId": "d2960a76-5a37-4ce2-e6ef-dd803a15606a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Non-Toxic Comments: 143346\n",
            "Toxic Comments: 15294\n",
            "Severe Toxic Comments: 1595\n",
            "Obscene Comments: 8449\n",
            "Threat Comments: 478\n",
            "Insult Comments: 7877\n",
            "Identity Hate Comments: 1405\n",
            "\n",
            "üîπ Non-Toxic Comments (First 5 comments):\n",
            "1. Explanation\n",
            "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
            "2. D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\n",
            "3. Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
            "4. \"\n",
            "More\n",
            "I can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\n",
            "\n",
            "There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"\n",
            "5. You, sir, are my hero. Any chance you remember what page that's on?\n",
            "\n",
            "üîπ Toxic Comments (First 5 comments):\n",
            "1. COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
            "2. Hey... what is it..\n",
            "@ | talk .\n",
            "What is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\n",
            "\n",
            "Ask Sityush to clean up his behavior than issue me nonsensical warnings...\n",
            "3. Bye! \n",
            "\n",
            "Don't look, come or think of comming back! Tosser.\n",
            "4. You are gay or antisemmitian? \n",
            "\n",
            "Archangel WHite Tiger\n",
            "\n",
            "Meow! Greetingshhh!\n",
            "\n",
            "Uh, there are two ways, why you do erased my comment about WW2, that holocaust was brutally slaying of Jews and not gays/Gypsys/Slavs/anyone...\n",
            "\n",
            "1 - If you are anti-semitian, than shave your head bald and go to the skinhead meetings!\n",
            "\n",
            "2 - If you doubt words of the Bible, that homosexuality is a deadly sin, make a pentagram tatoo on your forehead go to the satanistic masses with your gay pals!\n",
            "\n",
            "3 - First and last warning, you fucking gay - I won't appreciate if any more nazi shwain would write in my page! I don't wish to talk to you anymore!\n",
            "\n",
            "Beware of the Dark Side!\n",
            "5. FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!\n",
            "\n",
            "üîπ Severe Toxic Comments (First 5 comments):\n",
            "1. COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
            "2. Stupid peace of shit stop deleting my stuff asshole go die and fall in a hole go to hell!\n",
            "3. you are a stupid fuck \n",
            "\n",
            "and your mother's cunt stinks\n",
            "4. Hi \n",
            "\n",
            "Im a fucking bitch.\n",
            "\n",
            "50.180.208.181\n",
            "5. What a motherfucking piece of crap those fuckheads for blocking us!\n",
            "\n",
            "üîπ Obscene Comments (First 5 comments):\n",
            "1. COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
            "2. You are gay or antisemmitian? \n",
            "\n",
            "Archangel WHite Tiger\n",
            "\n",
            "Meow! Greetingshhh!\n",
            "\n",
            "Uh, there are two ways, why you do erased my comment about WW2, that holocaust was brutally slaying of Jews and not gays/Gypsys/Slavs/anyone...\n",
            "\n",
            "1 - If you are anti-semitian, than shave your head bald and go to the skinhead meetings!\n",
            "\n",
            "2 - If you doubt words of the Bible, that homosexuality is a deadly sin, make a pentagram tatoo on your forehead go to the satanistic masses with your gay pals!\n",
            "\n",
            "3 - First and last warning, you fucking gay - I won't appreciate if any more nazi shwain would write in my page! I don't wish to talk to you anymore!\n",
            "\n",
            "Beware of the Dark Side!\n",
            "3. FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!\n",
            "4. GET FUCKED UP. GET FUCKEEED UP.  GOT A DRINK THAT YOU CANT PUT DOWN???/ GET FUCK UP GET FUCKED UP.  I'M FUCKED UP RIGHT NOW!\n",
            "5. Stupid peace of shit stop deleting my stuff asshole go die and fall in a hole go to hell!\n",
            "\n",
            "üîπ Threat Comments (First 5 comments):\n",
            "1. Hi! I am back again!\n",
            "Last warning!\n",
            "Stop undoing my edits or die!\n",
            "2. I think that your a Fagget get a oife and burn in Hell I hate you 'm sorry we cant have any more sex i'm running out of conndoms\n",
            "3. I'm also a sock puppet of this account...SUPRISE!!\n",
            "-sincerely,\n",
            "            The man that will track you down from the Internet and kill you\n",
            "4. Fuck you, Smith. Please have me notified when you die. I want to dance on your grave.\n",
            "5. WOULDN'T BE THE FIRST TIME BITCH. FUCK YOU I'LL FIND OUT WHERE YOU LIVE, SODOMIZE YOUR WIFE AND THEN BURN YOUR HOUSE DOWN. FUCK YOU YOU FUCKING QUEER.\n",
            "\n",
            "üîπ Insult Comments (First 5 comments):\n",
            "1. COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
            "2. You are gay or antisemmitian? \n",
            "\n",
            "Archangel WHite Tiger\n",
            "\n",
            "Meow! Greetingshhh!\n",
            "\n",
            "Uh, there are two ways, why you do erased my comment about WW2, that holocaust was brutally slaying of Jews and not gays/Gypsys/Slavs/anyone...\n",
            "\n",
            "1 - If you are anti-semitian, than shave your head bald and go to the skinhead meetings!\n",
            "\n",
            "2 - If you doubt words of the Bible, that homosexuality is a deadly sin, make a pentagram tatoo on your forehead go to the satanistic masses with your gay pals!\n",
            "\n",
            "3 - First and last warning, you fucking gay - I won't appreciate if any more nazi shwain would write in my page! I don't wish to talk to you anymore!\n",
            "\n",
            "Beware of the Dark Side!\n",
            "3. FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!\n",
            "4. Stupid peace of shit stop deleting my stuff asshole go die and fall in a hole go to hell!\n",
            "5. =Tony Sidaway is obviously a fistfuckee. He loves an arm up his ass.\n",
            "\n",
            "üîπ Identity Hate Comments (First 5 comments):\n",
            "1. You are gay or antisemmitian? \n",
            "\n",
            "Archangel WHite Tiger\n",
            "\n",
            "Meow! Greetingshhh!\n",
            "\n",
            "Uh, there are two ways, why you do erased my comment about WW2, that holocaust was brutally slaying of Jews and not gays/Gypsys/Slavs/anyone...\n",
            "\n",
            "1 - If you are anti-semitian, than shave your head bald and go to the skinhead meetings!\n",
            "\n",
            "2 - If you doubt words of the Bible, that homosexuality is a deadly sin, make a pentagram tatoo on your forehead go to the satanistic masses with your gay pals!\n",
            "\n",
            "3 - First and last warning, you fucking gay - I won't appreciate if any more nazi shwain would write in my page! I don't wish to talk to you anymore!\n",
            "\n",
            "Beware of the Dark Side!\n",
            "2. A pair of jew-hating weiner nazi schmucks.\n",
            "3. I think that your a Fagget get a oife and burn in Hell I hate you 'm sorry we cant have any more sex i'm running out of conndoms\n",
            "4. Kill all niggers. \n",
            "\n",
            "I have hard, that others have said this.. should this be included? That racists sometimes say these.\n",
            "5. u r a tw@ fuck off u gay boy.U r smelly.Fuck ur mum poopie\n"
          ]
        }
      ],
      "source": [
        "# Create empty lists for each category\n",
        "non_toxic_comments = []\n",
        "toxic_comments = []\n",
        "severe_toxic_comments = []\n",
        "obscene_comments = []\n",
        "threat_comments = []\n",
        "insult_comments = []\n",
        "identity_hate_comments = []\n",
        "\n",
        "category_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "# Iterate through the dataset and categorize comments\n",
        "for index, row in train_df.iterrows():\n",
        "    comment = row['comment_text']\n",
        "\n",
        "    if row['toxic'] > 0:\n",
        "        toxic_comments.append(comment)\n",
        "    if row['severe_toxic'] > 0:\n",
        "        severe_toxic_comments.append(comment)\n",
        "    if row['obscene'] > 0:\n",
        "        obscene_comments.append(comment)\n",
        "    if row['threat'] > 0:\n",
        "        threat_comments.append(comment)\n",
        "    if row['insult'] > 0:\n",
        "        insult_comments.append(comment)\n",
        "    if row['identity_hate'] > 0:\n",
        "        identity_hate_comments.append(comment)\n",
        "\n",
        "    # If a comment doesn't belong to any category, classify it as non-toxic\n",
        "    if all(row[label] == 0 for label in category_labels):\n",
        "        non_toxic_comments.append(comment)\n",
        "\n",
        "# Display the count of comments in each category\n",
        "print(f\"Non-Toxic Comments: {len(non_toxic_comments)}\")\n",
        "print(f\"Toxic Comments: {len(toxic_comments)}\")\n",
        "print(f\"Severe Toxic Comments: {len(severe_toxic_comments)}\")\n",
        "print(f\"Obscene Comments: {len(obscene_comments)}\")\n",
        "print(f\"Threat Comments: {len(threat_comments)}\")\n",
        "print(f\"Insult Comments: {len(insult_comments)}\")\n",
        "print(f\"Identity Hate Comments: {len(identity_hate_comments)}\")\n",
        "\n",
        "# Print first 5 comments from each category\n",
        "def print_sample_comments(category_name, comments):\n",
        "    print(f\"\\nüîπ {category_name} (First 5 comments):\")\n",
        "    for i, comment in enumerate(comments[:5]):\n",
        "        print(f\"{i+1}. {comment}\")\n",
        "\n",
        "# Print sample comments for each category\n",
        "print_sample_comments(\"Non-Toxic Comments\", non_toxic_comments)\n",
        "print_sample_comments(\"Toxic Comments\", toxic_comments)\n",
        "print_sample_comments(\"Severe Toxic Comments\", severe_toxic_comments)\n",
        "print_sample_comments(\"Obscene Comments\", obscene_comments)\n",
        "print_sample_comments(\"Threat Comments\", threat_comments)\n",
        "print_sample_comments(\"Insult Comments\", insult_comments)\n",
        "print_sample_comments(\"Identity Hate Comments\", identity_hate_comments)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-unKxUJ39YK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rL5w2AkzYvc",
        "outputId": "284c777b-9768-4f33-81f2-91301feec3b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ Non-Toxic Comments (First 2 tokenized comments):\n",
            "1. ['explanation', 'why', 'the', 'edits', 'made', 'under', 'my', 'username', 'hardcore', 'metallica', 'fan', 'were', 'reverted', '?', 'they', 'were', \"n't\", 'vandalisms', ',', 'just', 'closure', 'on', 'some', 'gas', 'after', 'i', 'voted', 'at', 'new', 'york', 'dolls', 'fac', '.', 'and', 'please', 'do', \"n't\", 'remove', 'the', 'template', 'from', 'the', 'talk', 'page', 'since', 'i', \"'m\", 'retired', 'now.89.205.38.27']\n",
            "2. [\"d'aww\", '!', 'he', 'matches', 'this', 'background', 'colour', 'i', \"'m\", 'seemingly', 'stuck', 'with', '.', 'thanks', '.', '(', 'talk', ')', '21:51', ',', 'january', '11', ',', '2016', '(', 'utc', ')']\n",
            "\n",
            "üîπ Toxic Comments (First 2 tokenized comments):\n",
            "1. ['cocksucker', 'before', 'you', 'piss', 'around', 'on', 'my', 'work']\n",
            "2. ['hey', '...', 'what', 'is', 'it', '..', '@', '|', 'talk', '.', 'what', 'is', 'it', '...', 'an', 'exclusive', 'group', 'of', 'some', 'wp', 'talibans', '...', 'who', 'are', 'good', 'at', 'destroying', ',', 'self-appointed', 'purist', 'who', 'gang', 'up', 'any', 'one', 'who', 'asks', 'them', 'questions', 'abt', 'their', 'anti-social', 'and', 'destructive', '(', 'non', ')', '-contribution', 'at', 'wp', '?', 'ask', 'sityush', 'to', 'clean', 'up', 'his', 'behavior', 'than', 'issue', 'me', 'nonsensical', 'warnings', '...']\n",
            "\n",
            "üîπ Severe Toxic Comments (First 2 tokenized comments):\n",
            "1. ['cocksucker', 'before', 'you', 'piss', 'around', 'on', 'my', 'work']\n",
            "2. ['stupid', 'peace', 'of', 'shit', 'stop', 'deleting', 'my', 'stuff', 'asshole', 'go', 'die', 'and', 'fall', 'in', 'a', 'hole', 'go', 'to', 'hell', '!']\n",
            "\n",
            "üîπ Obscene Comments (First 2 tokenized comments):\n",
            "1. ['cocksucker', 'before', 'you', 'piss', 'around', 'on', 'my', 'work']\n",
            "2. ['you', 'are', 'gay', 'or', 'antisemmitian', '?', 'archangel', 'white', 'tiger', 'meow', '!', 'greetingshhh', '!', 'uh', ',', 'there', 'are', 'two', 'ways', ',', 'why', 'you', 'do', 'erased', 'my', 'comment', 'about', 'ww2', ',', 'that', 'holocaust', 'was', 'brutally', 'slaying', 'of', 'jews', 'and', 'not', 'gays/gypsys/slavs/anyone', '...', '1', '-', 'if', 'you', 'are', 'anti-semitian', ',', 'than', 'shave', 'your', 'head', 'bald', 'and', 'go', 'to', 'the', 'skinhead', 'meetings', '!', '2', '-', 'if', 'you', 'doubt', 'words', 'of', 'the', 'bible', ',', 'that', 'homosexuality', 'is', 'a', 'deadly', 'sin', ',', 'make', 'a', 'pentagram', 'tatoo', 'on', 'your', 'forehead', 'go', 'to', 'the', 'satanistic', 'masses', 'with', 'your', 'gay', 'pals', '!', '3', '-', 'first', 'and', 'last', 'warning', ',', 'you', 'fucking', 'gay', '-', 'i', 'wo', \"n't\", 'appreciate', 'if', 'any', 'more', 'nazi', 'shwain', 'would', 'write', 'in', 'my', 'page', '!', 'i', 'do', \"n't\", 'wish', 'to', 'talk', 'to', 'you', 'anymore', '!', 'beware', 'of', 'the', 'dark', 'side', '!']\n",
            "\n",
            "üîπ Threat Comments (First 2 tokenized comments):\n",
            "1. ['hi', '!', 'i', 'am', 'back', 'again', '!', 'last', 'warning', '!', 'stop', 'undoing', 'my', 'edits', 'or', 'die', '!']\n",
            "2. ['i', 'think', 'that', 'your', 'a', 'fagget', 'get', 'a', 'oife', 'and', 'burn', 'in', 'hell', 'i', 'hate', 'you', \"'m\", 'sorry', 'we', 'cant', 'have', 'any', 'more', 'sex', 'i', \"'m\", 'running', 'out', 'of', 'conndoms']\n",
            "\n",
            "üîπ Insult Comments (First 2 tokenized comments):\n",
            "1. ['cocksucker', 'before', 'you', 'piss', 'around', 'on', 'my', 'work']\n",
            "2. ['you', 'are', 'gay', 'or', 'antisemmitian', '?', 'archangel', 'white', 'tiger', 'meow', '!', 'greetingshhh', '!', 'uh', ',', 'there', 'are', 'two', 'ways', ',', 'why', 'you', 'do', 'erased', 'my', 'comment', 'about', 'ww2', ',', 'that', 'holocaust', 'was', 'brutally', 'slaying', 'of', 'jews', 'and', 'not', 'gays/gypsys/slavs/anyone', '...', '1', '-', 'if', 'you', 'are', 'anti-semitian', ',', 'than', 'shave', 'your', 'head', 'bald', 'and', 'go', 'to', 'the', 'skinhead', 'meetings', '!', '2', '-', 'if', 'you', 'doubt', 'words', 'of', 'the', 'bible', ',', 'that', 'homosexuality', 'is', 'a', 'deadly', 'sin', ',', 'make', 'a', 'pentagram', 'tatoo', 'on', 'your', 'forehead', 'go', 'to', 'the', 'satanistic', 'masses', 'with', 'your', 'gay', 'pals', '!', '3', '-', 'first', 'and', 'last', 'warning', ',', 'you', 'fucking', 'gay', '-', 'i', 'wo', \"n't\", 'appreciate', 'if', 'any', 'more', 'nazi', 'shwain', 'would', 'write', 'in', 'my', 'page', '!', 'i', 'do', \"n't\", 'wish', 'to', 'talk', 'to', 'you', 'anymore', '!', 'beware', 'of', 'the', 'dark', 'side', '!']\n",
            "\n",
            "üîπ Identity Hate Comments (First 2 tokenized comments):\n",
            "1. ['you', 'are', 'gay', 'or', 'antisemmitian', '?', 'archangel', 'white', 'tiger', 'meow', '!', 'greetingshhh', '!', 'uh', ',', 'there', 'are', 'two', 'ways', ',', 'why', 'you', 'do', 'erased', 'my', 'comment', 'about', 'ww2', ',', 'that', 'holocaust', 'was', 'brutally', 'slaying', 'of', 'jews', 'and', 'not', 'gays/gypsys/slavs/anyone', '...', '1', '-', 'if', 'you', 'are', 'anti-semitian', ',', 'than', 'shave', 'your', 'head', 'bald', 'and', 'go', 'to', 'the', 'skinhead', 'meetings', '!', '2', '-', 'if', 'you', 'doubt', 'words', 'of', 'the', 'bible', ',', 'that', 'homosexuality', 'is', 'a', 'deadly', 'sin', ',', 'make', 'a', 'pentagram', 'tatoo', 'on', 'your', 'forehead', 'go', 'to', 'the', 'satanistic', 'masses', 'with', 'your', 'gay', 'pals', '!', '3', '-', 'first', 'and', 'last', 'warning', ',', 'you', 'fucking', 'gay', '-', 'i', 'wo', \"n't\", 'appreciate', 'if', 'any', 'more', 'nazi', 'shwain', 'would', 'write', 'in', 'my', 'page', '!', 'i', 'do', \"n't\", 'wish', 'to', 'talk', 'to', 'you', 'anymore', '!', 'beware', 'of', 'the', 'dark', 'side', '!']\n",
            "2. ['a', 'pair', 'of', 'jew-hating', 'weiner', 'nazi', 'schmucks', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the 'punkt_tab' data if not already present\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Function to tokenize a list of comments\n",
        "def tokenize_comments(comments):\n",
        "    return [word_tokenize(comment.lower()) for comment in comments]  # Convert to lowercase before tokenization\n",
        "\n",
        "# ... (rest of your code)\n",
        "# Apply tokenization to each category\n",
        "tokenized_non_toxic = tokenize_comments(non_toxic_comments)\n",
        "tokenized_toxic = tokenize_comments(toxic_comments)\n",
        "tokenized_severe_toxic = tokenize_comments(severe_toxic_comments)\n",
        "tokenized_obscene = tokenize_comments(obscene_comments)\n",
        "tokenized_threat = tokenize_comments(threat_comments)\n",
        "tokenized_insult = tokenize_comments(insult_comments)\n",
        "tokenized_identity_hate = tokenize_comments(identity_hate_comments)\n",
        "\n",
        "# Print first 2 tokenized comments from each category\n",
        "def print_sample_tokens(category_name, tokenized_comments):\n",
        "    print(f\"\\nüîπ {category_name} (First 2 tokenized comments):\")\n",
        "    for i, tokens in enumerate(tokenized_comments[:2]):\n",
        "        print(f\"{i+1}. {tokens}\")\n",
        "\n",
        "print_sample_tokens(\"Non-Toxic Comments\", tokenized_non_toxic)\n",
        "print_sample_tokens(\"Toxic Comments\", tokenized_toxic)\n",
        "print_sample_tokens(\"Severe Toxic Comments\", tokenized_severe_toxic)\n",
        "print_sample_tokens(\"Obscene Comments\", tokenized_obscene)\n",
        "print_sample_tokens(\"Threat Comments\", tokenized_threat)\n",
        "print_sample_tokens(\"Insult Comments\", tokenized_insult)\n",
        "print_sample_tokens(\"Identity Hate Comments\", tokenized_identity_hate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCS1xA-J3-Yp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXn1l6uz1MRv",
        "outputId": "2ea78c1b-bc2d-4c0d-c591-a7e2fb7c546e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ Non-Toxic Comments\n",
            "  - Total Tokens: 11765071\n",
            "  - Unique Tokens: 244141\n",
            "  - Sample Unique Words: ['rfas', 'carsulae', 'biderman', 'aeneidservius', 'grye', 'ns', 'acquisitions', 'sambur', '06:49:24', 'kellers']\n",
            "\n",
            "üîπ Toxic Comments\n",
            "  - Total Tokens: 965830\n",
            "  - Unique Tokens: 38114\n",
            "  - Sample Unique Words: ['‚ïü‚îÄtag‚ñ∫africa', '207.97.213.169', 'rfas', 'professionnal', 'evolutionist', 'confused', 'weller', 'spilling', 'built', 'reggie']\n",
            "\n",
            "üîπ Severe Toxic Comments\n",
            "  - Total Tokens: 161121\n",
            "  - Unique Tokens: 6447\n",
            "  - Sample Unique Words: ['pratting', 'noticing', 'amaze', 'bag/cok', '92.232.0.21', 'weller', 'built', 'ag', 'amature', '(']\n",
            "\n",
            "üîπ Obscene Comments\n",
            "  - Total Tokens: 514086\n",
            "  - Unique Tokens: 24312\n",
            "  - Sample Unique Words: ['‚ïü‚îÄtag‚ñ∫africa', 'confused', 'weller', 'spilling', 'built', 'amendmants', 'phalus', 'durita', 'amature', '(']\n",
            "\n",
            "üîπ Threat Comments\n",
            "  - Total Tokens: 38647\n",
            "  - Unique Tokens: 3151\n",
            "  - Sample Unique Words: ['kickin', 'replies', \"'ll\", 'harrass', 'franca', 'well', 'smelly', 'why', 'skank', 'aqua']\n",
            "\n",
            "üîπ Insult Comments\n",
            "  - Total Tokens: 467647\n",
            "  - Unique Tokens: 22595\n",
            "  - Sample Unique Words: ['207.97.213.169', 'confused', 'weller', 'built', 'mil', 'amature', '(', 'fdb', 'targeted', 'wikians']\n",
            "\n",
            "üîπ Identity Hate Comments\n",
            "  - Total Tokens: 85955\n",
            "  - Unique Tokens: 7897\n",
            "  - Sample Unique Words: ['swinging', 'reportin', 'amaze', 'confused', 'ewww', 'bag/cok', '92.232.0.21', 'fuhrer', 'parker', '(']\n",
            "\n",
            "üîπ Non-Toxic Comments\n",
            "  - Total Tokens: 11765071\n",
            "  - Unique Tokens: 244141\n",
            "  - Sample Unique Words: ['rfas', 'carsulae', 'biderman', 'aeneidservius', 'grye', 'ns', 'acquisitions', 'sambur', '06:49:24', 'kellers']\n",
            "\n",
            "üîπ Toxic Comments\n",
            "  - Total Tokens: 965830\n",
            "  - Unique Tokens: 38114\n",
            "  - Sample Unique Words: ['‚ïü‚îÄtag‚ñ∫africa', '207.97.213.169', 'rfas', 'professionnal', 'evolutionist', 'confused', 'weller', 'spilling', 'built', 'reggie']\n",
            "\n",
            "üîπ Severe Toxic Comments\n",
            "  - Total Tokens: 161121\n",
            "  - Unique Tokens: 6447\n",
            "  - Sample Unique Words: ['pratting', 'noticing', 'amaze', 'bag/cok', '92.232.0.21', 'weller', 'built', 'ag', 'amature', '(']\n",
            "\n",
            "üîπ Obscene Comments\n",
            "  - Total Tokens: 514086\n",
            "  - Unique Tokens: 24312\n",
            "  - Sample Unique Words: ['‚ïü‚îÄtag‚ñ∫africa', 'confused', 'weller', 'spilling', 'built', 'amendmants', 'phalus', 'durita', 'amature', '(']\n",
            "\n",
            "üîπ Threat Comments\n",
            "  - Total Tokens: 38647\n",
            "  - Unique Tokens: 3151\n",
            "  - Sample Unique Words: ['kickin', 'replies', \"'ll\", 'harrass', 'franca', 'well', 'smelly', 'why', 'skank', 'aqua']\n",
            "\n",
            "üîπ Insult Comments\n",
            "  - Total Tokens: 467647\n",
            "  - Unique Tokens: 22595\n",
            "  - Sample Unique Words: ['207.97.213.169', 'confused', 'weller', 'built', 'mil', 'amature', '(', 'fdb', 'targeted', 'wikians']\n",
            "\n",
            "üîπ Identity Hate Comments\n",
            "  - Total Tokens: 85955\n",
            "  - Unique Tokens: 7897\n",
            "  - Sample Unique Words: ['swinging', 'reportin', 'amaze', 'confused', 'ewww', 'bag/cok', '92.232.0.21', 'fuhrer', 'parker', '(']\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Function to analyze token statistics\n",
        "def analyze_tokens(tokenized_comments):\n",
        "    all_tokens = [token for comment in tokenized_comments for token in comment]  # Flatten list\n",
        "    unique_tokens = set(all_tokens)  # Get unique words\n",
        "    return len(all_tokens), len(unique_tokens), unique_tokens  # Return total, unique count, and unique words\n",
        "\n",
        "# Analyze token statistics for each category\n",
        "categories = {\n",
        "    \"Non-Toxic Comments\": tokenized_non_toxic,\n",
        "    \"Toxic Comments\": tokenized_toxic,\n",
        "    \"Severe Toxic Comments\": tokenized_severe_toxic,\n",
        "    \"Obscene Comments\": tokenized_obscene,\n",
        "    \"Threat Comments\": tokenized_threat,\n",
        "    \"Insult Comments\": tokenized_insult,\n",
        "    \"Identity Hate Comments\": tokenized_identity_hate\n",
        "}\n",
        "\n",
        "# Print token statistics\n",
        "for category, tokens in categories.items():\n",
        "    total_tokens, unique_count, unique_words = analyze_tokens(tokens)\n",
        "    print(f\"\\nüîπ {category}\")\n",
        "    print(f\"  - Total Tokens: {total_tokens}\")\n",
        "    print(f\"  - Unique Tokens: {unique_count}\")\n",
        "    print(f\"  - Sample Unique Words: {list(unique_words)[:10]}\")  # Display 10 unique words\n",
        "from collections import Counter\n",
        "\n",
        "# Function to analyze token statistics\n",
        "def analyze_tokens(tokenized_comments):\n",
        "    all_tokens = [token for comment in tokenized_comments for token in comment]  # Flatten list\n",
        "    unique_tokens = set(all_tokens)  # Get unique words\n",
        "    return len(all_tokens), len(unique_tokens), unique_tokens  # Return total, unique count, and unique words\n",
        "\n",
        "# Analyze token statistics for each category\n",
        "categories = {\n",
        "    \"Non-Toxic Comments\": tokenized_non_toxic,\n",
        "    \"Toxic Comments\": tokenized_toxic,\n",
        "    \"Severe Toxic Comments\": tokenized_severe_toxic,\n",
        "    \"Obscene Comments\": tokenized_obscene,\n",
        "    \"Threat Comments\": tokenized_threat,\n",
        "    \"Insult Comments\": tokenized_insult,\n",
        "    \"Identity Hate Comments\": tokenized_identity_hate\n",
        "}\n",
        "\n",
        "# Print token statistics\n",
        "for category, tokens in categories.items():\n",
        "    total_tokens, unique_count, unique_words = analyze_tokens(tokens)\n",
        "    print(f\"\\nüîπ {category}\")\n",
        "    print(f\"  - Total Tokens: {total_tokens}\")\n",
        "    print(f\"  - Unique Tokens: {unique_count}\")\n",
        "    print(f\"  - Sample Unique Words: {list(unique_words)[:10]}\")  # Display 10 unique words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZGRXc-R3_lm",
        "outputId": "bb5060e1-1fee-4e75-da3e-a94be8c4b0e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ Toxic (First 10 refined tokens):\n",
            "[]\n",
            "\n",
            "üîπ Severe Toxic (First 10 refined tokens):\n",
            "[]\n",
            "\n",
            "üîπ Obscene (First 10 refined tokens):\n",
            "['cocksuck', 'vagoo', 'heeee', 'knowledge-sharing', 'favourites.', 'durita', 'dx927', 'gf108', 'c·ª©', 'inmediatly']\n",
            "\n",
            "üîπ Threat (First 10 refined tokens):\n",
            "['antiviolence', 'repulsiveness', 'cyber-cat', 'eductational', 'gayreeks', 'weare', 'padmalskhmi', 'him/hersef', '.72.76.10.207', 'psuedointellectuals']\n",
            "\n",
            "üîπ Insult (First 10 refined tokens):\n",
            "['5men', 'mizraim', 'decieves', 'time/era', 'meaasges', 'enimy', 'pro-spam', '◊ï÷º◊û÷¥◊¶÷∞◊®÷∑◊ô÷¥◊ù', 'manis', 'heeee']\n",
            "\n",
            "üîπ Identity Hate (First 10 refined tokens):\n",
            "['=69701704', 'apolegetic', 'gurdan', \"'romance\", 'difference.we', 'time/era', 'mohammedens', 'bolshevist', 'ossmann', 'pro-spam']\n"
          ]
        }
      ],
      "source": [
        "# Get unique tokens for each category\n",
        "unique_non_toxic_tokens = analyze_tokens(tokenized_non_toxic)[2]\n",
        "unique_toxic_tokens = analyze_tokens(tokenized_toxic)[2]\n",
        "unique_severe_toxic_tokens = analyze_tokens(tokenized_severe_toxic)[2]\n",
        "unique_obscene_tokens = analyze_tokens(tokenized_obscene)[2]\n",
        "unique_threat_tokens = analyze_tokens(tokenized_threat)[2]\n",
        "unique_insult_tokens = analyze_tokens(tokenized_insult)[2]\n",
        "unique_identity_hate_tokens = analyze_tokens(tokenized_identity_hate)[2]\n",
        "\n",
        "# Function to refine tokens by removing common ones with non-toxic and toxic categories\n",
        "def refine_tokens(category_tokens):\n",
        "    common_tokens = unique_non_toxic_tokens.union(unique_toxic_tokens)  # Common with both non-toxic and toxic\n",
        "    return category_tokens - common_tokens  # Remove them\n",
        "\n",
        "# Apply refinement to all toxic categories\n",
        "refined_toxic_tokens = refine_tokens(unique_toxic_tokens)\n",
        "refined_severe_toxic_tokens = refine_tokens(unique_severe_toxic_tokens)\n",
        "refined_obscene_tokens = refine_tokens(unique_obscene_tokens)\n",
        "refined_threat_tokens = refine_tokens(unique_threat_tokens)\n",
        "refined_insult_tokens = refine_tokens(unique_insult_tokens)\n",
        "refined_identity_hate_tokens = refine_tokens(unique_identity_hate_tokens)\n",
        "\n",
        "# Print refined tokens for each category\n",
        "def print_refined_tokens(category_name, refined_tokens):\n",
        "    print(f\"\\nüîπ {category_name} (First 10 refined tokens):\")\n",
        "    print(list(refined_tokens)[:10])\n",
        "\n",
        "print_refined_tokens(\"Toxic\", refined_toxic_tokens)\n",
        "print_refined_tokens(\"Severe Toxic\", refined_severe_toxic_tokens)\n",
        "print_refined_tokens(\"Obscene\", refined_obscene_tokens)\n",
        "print_refined_tokens(\"Threat\", refined_threat_tokens)\n",
        "print_refined_tokens(\"Insult\", refined_insult_tokens)\n",
        "print_refined_tokens(\"Identity Hate\", refined_identity_hate_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKD9IjTF4riM",
        "outputId": "fa254c77-e117-45cc-e943-31e1d32c09fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ Toxic - Top 10 tokens by probability:\n",
            "!: 0.057508\n",
            "you: 0.039329\n",
            ".: 0.035712\n",
            ",: 0.028964\n",
            "i: 0.023464\n",
            "the: 0.021200\n",
            "a: 0.021112\n",
            "and: 0.016267\n",
            "to: 0.016213\n",
            "is: 0.013314\n",
            "\n",
            "üîπ Severe Toxic - Top 10 tokens by probability:\n",
            "!: 0.161121\n",
            "you: 0.064424\n",
            "fuck: 0.041217\n",
            ".: 0.023132\n",
            "i: 0.022902\n",
            ",: 0.022610\n",
            "a: 0.018886\n",
            "suck: 0.015715\n",
            "the: 0.013338\n",
            "ass: 0.013065\n",
            "\n",
            "üîπ Obscene - Top 10 tokens by probability:\n",
            "!: 0.054730\n",
            "you: 0.045615\n",
            ".: 0.035547\n",
            ",: 0.028653\n",
            "i: 0.023245\n",
            "a: 0.022487\n",
            "the: 0.020786\n",
            "fuck: 0.017820\n",
            "and: 0.016042\n",
            "to: 0.014321\n",
            "\n",
            "üîπ Threat - Top 10 tokens by probability:\n",
            "!: 0.208063\n",
            "i: 0.054312\n",
            "you: 0.052501\n",
            ".: 0.044143\n",
            "die: 0.030196\n",
            ",: 0.028411\n",
            "will: 0.021735\n",
            "ass: 0.019898\n",
            "to: 0.016690\n",
            "and: 0.015551\n",
            "\n",
            "üîπ Insult - Top 10 tokens by probability:\n",
            "!: 0.062853\n",
            "you: 0.051013\n",
            ".: 0.034406\n",
            ",: 0.027818\n",
            "a: 0.024944\n",
            "i: 0.022243\n",
            "the: 0.018426\n",
            "and: 0.016123\n",
            "fuck: 0.015501\n",
            "to: 0.013666\n",
            "\n",
            "üîπ Identity Hate - Top 10 tokens by probability:\n",
            "!: 0.037787\n",
            "nigger: 0.034483\n",
            "you: 0.034076\n",
            ".: 0.029329\n",
            ",: 0.024943\n",
            "a: 0.024141\n",
            "is: 0.022151\n",
            "i: 0.020266\n",
            "and: 0.018789\n",
            "the: 0.016346\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Function to calculate token frequency and probability for a given category\n",
        "def calculate_token_probabilities(tokenized_comments):\n",
        "    token_counts = Counter([token for comment in tokenized_comments for token in comment])  # Count tokens\n",
        "    total_tokens = sum(token_counts.values())  # Total tokens in category\n",
        "    token_probabilities = {token: count / total_tokens for token, count in token_counts.items()}  # Probability\n",
        "    return token_probabilities\n",
        "\n",
        "# Calculate token probabilities for each category\n",
        "toxic_token_probs = calculate_token_probabilities(tokenized_toxic)\n",
        "severe_toxic_token_probs = calculate_token_probabilities(tokenized_severe_toxic)\n",
        "obscene_token_probs = calculate_token_probabilities(tokenized_obscene)\n",
        "threat_token_probs = calculate_token_probabilities(tokenized_threat)\n",
        "insult_token_probs = calculate_token_probabilities(tokenized_insult)\n",
        "identity_hate_token_probs = calculate_token_probabilities(tokenized_identity_hate)\n",
        "\n",
        "# Function to print top N tokens with highest probability in a category\n",
        "def print_top_tokens(category_name, token_probs, top_n=10):\n",
        "    sorted_tokens = sorted(token_probs.items(), key=lambda x: x[1], reverse=True)  # Sort by probability\n",
        "    print(f\"\\nüîπ {category_name} - Top {top_n} tokens by probability:\")\n",
        "    for token, prob in sorted_tokens[:top_n]:\n",
        "        print(f\"{token}: {prob:.6f}\")\n",
        "\n",
        "# Print top tokens for each category\n",
        "print_top_tokens(\"Toxic\", toxic_token_probs)\n",
        "print_top_tokens(\"Severe Toxic\", severe_toxic_token_probs)\n",
        "print_top_tokens(\"Obscene\", obscene_token_probs)\n",
        "print_top_tokens(\"Threat\", threat_token_probs)\n",
        "print_top_tokens(\"Insult\", insult_token_probs)\n",
        "print_top_tokens(\"Identity Hate\", identity_hate_token_probs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1QcQMAkl8TkK"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Function to tokenize a list of comments\n",
        "def tokenize_comments(comments):\n",
        "    return [word_tokenize(comment.lower()) for comment in comments]  # Convert to lowercase before tokenization\n",
        "\n",
        "\n",
        "# Tokenize each category's comments\n",
        "tokenized_non_toxic = tokenize_comments(non_toxic_comments)\n",
        "tokenized_toxic = tokenize_comments(toxic_comments)\n",
        "tokenized_severe_toxic = tokenize_comments(severe_toxic_comments)\n",
        "tokenized_obscene = tokenize_comments(obscene_comments)\n",
        "tokenized_threat = tokenize_comments(threat_comments)\n",
        "tokenized_insult = tokenize_comments(insult_comments)\n",
        "tokenized_identity_hate = tokenize_comments(identity_hate_comments)\n",
        "\n",
        "# Function to calculate prior probabilities\n",
        "def calculate_prior_probabilities(categories):\n",
        "    total_comments = sum(len(category) for category in categories.values())\n",
        "    prior_probs = {}\n",
        "\n",
        "    for category, comments in categories.items():\n",
        "        prior_probs[category] = len(comments) / total_comments\n",
        "\n",
        "    return prior_probs\n",
        "\n",
        "# Function to calculate token probabilities for each category (likelihood)\n",
        "def calculate_token_probabilities(tokenized_comments):\n",
        "    all_tokens = [token for comment in tokenized_comments for token in comment]\n",
        "    token_counts = Counter(all_tokens)\n",
        "    total_tokens = len(all_tokens)\n",
        "    token_probs = {token: count / total_tokens for token, count in token_counts.items()}\n",
        "\n",
        "    return token_probs, total_tokens\n",
        "\n",
        "# Function to calculate Bayes' Theorem and classify a new comment\n",
        "def classify_comment(comment, prior_probs, token_probs, categories, vocab_size):\n",
        "    comment_tokens = word_tokenize(comment.lower())\n",
        "\n",
        "    category_scores = {}\n",
        "\n",
        "    for category in categories:\n",
        "        score = np.log(prior_probs[category])  # Start with the prior probability\n",
        "\n",
        "        for token in comment_tokens:\n",
        "            token_prob = token_probs[category].get(token, 1 / (vocab_size * 1))  # Laplace smoothing\n",
        "            score += np.log(token_prob)\n",
        "\n",
        "        category_scores[category] = score\n",
        "\n",
        "    predicted_category = max(category_scores, key=category_scores.get)\n",
        "    return predicted_category\n",
        "\n",
        "# Step 1: Calculate Prior Probabilities\n",
        "categories = {\n",
        "    'non_toxic': non_toxic_comments,\n",
        "    'toxic': toxic_comments,\n",
        "    'severe_toxic': severe_toxic_comments,\n",
        "    'obscene': obscene_comments,\n",
        "    'threat': threat_comments,\n",
        "    'insult': insult_comments,\n",
        "    'identity_hate': identity_hate_comments\n",
        "}\n",
        "\n",
        "prior_probs = calculate_prior_probabilities(categories)\n",
        "\n",
        "# Step 2: Calculate Token Probabilities for each category\n",
        "token_probs = {}\n",
        "vocab_size = 0\n",
        "\n",
        "for category, tokenized_comments in zip(categories.keys(), [tokenized_non_toxic, tokenized_toxic, tokenized_severe_toxic, tokenized_obscene, tokenized_threat, tokenized_insult, tokenized_identity_hate]):\n",
        "    token_probs[category], _ = calculate_token_probabilities(tokenized_comments)\n",
        "\n",
        "# Combine all token counts to calculate the overall vocabulary size\n",
        "all_tokens = [token for tokenized_comments in [tokenized_non_toxic, tokenized_toxic, tokenized_severe_toxic, tokenized_obscene, tokenized_threat, tokenized_insult, tokenized_identity_hate] for comment in tokenized_comments for token in comment]\n",
        "vocab_size = len(set(all_tokens))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHH46lFs9zEV",
        "outputId": "3496a127-acf1-4089-ef0a-d4c00dfad25c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a comment to predict its category: die goo kill\n",
            "Predicted Category: threat\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Example of classifying a new comment\n",
        "new_comment = input(\"Enter a comment to predict its category: \")\n",
        "predicted_category = classify_comment(new_comment, prior_probs, token_probs, categories.keys(), vocab_size)\n",
        "\n",
        "print(f\"Predicted Category: {predicted_category}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSb8KEYE84QF",
        "outputId": "c5b44351-cca6-4b5a-a851-c8cadc4648c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 0.7325\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "identity_hate       0.00      0.14      0.00        14\n",
            "       insult       0.00      0.06      0.00        68\n",
            "    non_toxic       0.99      0.75      0.85    146921\n",
            "      obscene       0.00      0.15      0.00        65\n",
            " severe_toxic       0.00      0.00      0.00         0\n",
            "       threat       0.00      0.17      0.00         6\n",
            "        toxic       0.10      0.41      0.16      6090\n",
            "\n",
            "     accuracy                           0.73    153164\n",
            "    macro avg       0.16      0.24      0.15    153164\n",
            " weighted avg       0.95      0.73      0.82    153164\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load test dataset\n",
        "test_path = 'Dataset/test.csv'\n",
        "test_labels_path = 'Dataset/test_labels.csv'\n",
        "test_df = pd.read_csv(test_path)\n",
        "test_labels_df = pd.read_csv(test_labels_path)\n",
        "\n",
        "# Merge test data and labels on 'id'\n",
        "test_df = test_df.merge(test_labels_df, on='id')\n",
        "\n",
        "# Function to determine the actual category of a comment\n",
        "def get_actual_category(row):\n",
        "    categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "    # If all labels are ‚â§ 0, it's non-toxic\n",
        "    if all(row[category] <= 0 for category in categories):\n",
        "        return 'non_toxic'\n",
        "\n",
        "    # Otherwise, return the most severe category (you can change this logic if needed)\n",
        "    for category in categories:\n",
        "        if row[category] == 1:\n",
        "            return category\n",
        "    return 'non_toxic'  # Default fallback\n",
        "\n",
        "# Apply the function to get actual categories\n",
        "test_df['actual_category'] = test_df.apply(get_actual_category, axis=1)\n",
        "\n",
        "# Predict categories using your model\n",
        "predicted_categories = [classify_comment(comment, prior_probs, token_probs, categories.keys(), vocab_size)\n",
        "                        for comment in test_df['comment_text']]\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = accuracy_score(test_df['actual_category'], predicted_categories)\n",
        "report = classification_report(test_df['actual_category'], predicted_categories)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FVDZ6LZGHm0",
        "outputId": "9e6005ed-9811-4d36-adcd-34d517e891b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved as comment_classifier.pkl\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Define model components to save\n",
        "model_data = {\n",
        "    \"prior_probs\": prior_probs,\n",
        "    \"token_probs\": token_probs,\n",
        "    \"categories\": list(categories.keys()),\n",
        "    \"vocab_size\": vocab_size\n",
        "}\n",
        "\n",
        "# Save the model using Pickle\n",
        "model_path = \"comment_classifier.pkl\"\n",
        "with open(model_path, \"wb\") as model_file:\n",
        "    pickle.dump(model_data, model_file)\n",
        "\n",
        "print(f\"Model saved as {model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM-joLBeGOp5",
        "outputId": "cee7adf2-3b6c-4b7e-a581-96cec14a6846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Categories: ['non_toxic', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
          ]
        }
      ],
      "source": [
        "print(\"Model Categories:\", list(categories.keys()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRf7NZlwuGJZ"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
