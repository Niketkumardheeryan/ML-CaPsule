{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XgBoost.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_oVPxasDj5e"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlff6lQDHURB"
      },
      "source": [
        "**Libraries Used**\n",
        "\n",
        "* **tqdm** is a Python library that allows you to output a smart progress bar by wrapping around any iterable.\n",
        "\n",
        "* **OS** module in Python provides functions for interacting with the operating system. OS comes under Python's standard utility modules. This module provides a portable way of using operating system dependent functionality.\n",
        "\n",
        "* **NumPy** is the fundamental package for scientific computing in Python.\n",
        "\n",
        "**Building function for XgBoost**\n",
        "\n",
        "* We know that boosting algorithm needs a base model and then with the help of base model it moves forward for improved results.\n",
        "\n",
        "* So first we will pass argument such as, n_estimators,max_depth,learning_rate,lambda.\n",
        "\n",
        "**Structure**\n",
        "\n",
        "\n",
        "\n",
        "1. First we will define main function under class XGBoost.\n",
        "2. Then will make functions such as fir, train,predict which will do the respective task assinged to them.\n",
        "3. Then we will build the **XGBoost Regressor** function.\n",
        "4. Lastly we will build the **XGBoost Classifier** function. While doing Classfication in XGBoost we need to calculate the **probability** initailly so there will be addition of one extra function probability as compared to regressor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg3S5cwPFglW"
      },
      "source": [
        "class XGBoost:\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_estimators=30,\n",
        "                 max_depth=10,\n",
        "                 learning_rate=0.1,\n",
        "                 min_samples_split=2,\n",
        "                 reg_lambda=1,\n",
        "                 min_impurity=1e-7,\n",
        "                 min_child_weight=1,\n",
        "                 sub_sample=0.8,\n",
        "                 sub_feature=0.8,\n",
        "                 n_jobs=1,\n",
        "                 random_state=None\n",
        "                 ):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_impurity = min_impurity\n",
        "        self.max_depth = max_depth\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.min_child_weight = min_child_weight\n",
        "        self.sub_sample = sub_sample\n",
        "        self.sub_feature = sub_feature\n",
        "        self.n_jobs = n_jobs if n_jobs <= os.cpu_count() else os.cpu_count()\n",
        "        self.random_state = random_state\n",
        "    \n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.__initalize()\n",
        "        self.__sample(X)\n",
        "        y_pred = np.ones_like(y)\n",
        "\n",
        "        for i in tqdm(range(self.n_estimators)):\n",
        "            tree = self.estimators[i]\n",
        "            sample_index = self.shuffle_sample[i]\n",
        "            feture_index = self.shuffle_feture[i]\n",
        "            sub_X = X[sample_index, :][:, feture_index]\n",
        "            sub_y = y[sample_index]\n",
        "            sub_y_pred = y_pred[sample_index]\n",
        "            y_and_pred = np.concatenate((sub_y, sub_y_pred), axis=1)\n",
        "            tree.fit(sub_X, y_and_pred)\n",
        "            update_pred = tree.predict(X[:, feture_index])\n",
        "            y_pred += np.multiply(self.learning_rate, update_pred)\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def train(self, X, y,val_X,val_y,metrics='default'):\n",
        "\n",
        "        self.__initalize()\n",
        "        self.__sample(X)\n",
        "\n",
        "        y_pred = np.ones_like(y)\n",
        "        val_y_pred=np.ones_like(val_y)\n",
        "        current_metrics=-np.inf\n",
        "        for i in tqdm(range(self.n_estimators)):\n",
        "            tree = self.estimators[i]\n",
        "            sample_index = self.shuffle_sample[i]\n",
        "            feture_index = self.shuffle_feture[i]\n",
        "            sub_X = X[sample_index, :][:, feture_index]\n",
        "            sub_y = y[sample_index]\n",
        "            sub_y_pred = y_pred[sample_index]\n",
        "            y_and_pred = np.concatenate((sub_y, sub_y_pred), axis=1)\n",
        "            tree.fit(sub_X, y_and_pred)\n",
        "            update_pred = tree.predict(X[:, feture_index])\n",
        "            y_pred += np.multiply(self.learning_rate, update_pred)\n",
        "\n",
        "            update_val_y_pred=tree.predict(val_X[:, feture_index])\n",
        "            val_y_pred+= np.multiply(self.learning_rate, update_val_y_pred)\n",
        "            this_metric=self.loss.cost(val_y,val_y_pred)\n",
        "            orig=self.loss.cost(y,y_pred)\n",
        "            print(orig,this_metric)\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = None\n",
        "\n",
        "        for i, tree in enumerate(self.estimators):\n",
        "\n",
        "            feture_index = self.shuffle_feture[i]\n",
        "\n",
        "            sub_X = X[:, feture_index]\n",
        "            update_pred = tree.predict(sub_X)\n",
        "            if y_pred is None:\n",
        "                y_pred = np.ones_like(update_pred)\n",
        "            y_pred += np.multiply(self.learning_rate, update_pred)\n",
        "\n",
        "        return y_pred"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8D6uct8Fk_p"
      },
      "source": [
        "class XGBRegressor(XGBoost):\n",
        "    def __init__(self, n_estimators=30,\n",
        "                 max_depth=8,\n",
        "                 learning_rate=1,\n",
        "                 min_samples_split=2,\n",
        "                 min_impurity=1e-7,\n",
        "                 min_child_weight=1,\n",
        "                 sub_sample=0.8,\n",
        "                 sub_feature=0.8,\n",
        "                 reg_lambda=1,\n",
        "                 n_jobs=1,\n",
        "                 random_state=None):\n",
        "        super().__init__(n_estimators=n_estimators,\n",
        "                         max_depth=max_depth,\n",
        "                         learning_rate=learning_rate,\n",
        "                         min_samples_split=min_samples_split,\n",
        "                         min_impurity=min_impurity,\n",
        "                         min_child_weight=min_child_weight,\n",
        "                         sub_sample=sub_sample,\n",
        "                         sub_feature=sub_feature,\n",
        "                         reg_lambda=reg_lambda,\n",
        "                         n_jobs=n_jobs,\n",
        "                         random_state=random_state)\n",
        "      \n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y = y[:, np.newaxis]\n",
        "        super().fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = super().predict(X)\n",
        "        return y_pred.flatten()\n",
        "\n",
        "\n",
        "    def train(self, X, y,val_data,metrics='default'):\n",
        "        val_X,val_y=val_data\n",
        "        y = y[:, np.newaxis]\n",
        "        val_y=val_y[:, np.newaxis]\n",
        "        super().train(X,y,val_X,val_y)\n",
        "        return self"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gum24uSSFyBc"
      },
      "source": [
        "class XGBClassifier(XGBoost):\n",
        "    def __init__(self, n_estimators=30,\n",
        "                 max_depth=8,\n",
        "                 learning_rate=0.3,\n",
        "                 min_samples_split=2,\n",
        "                 min_impurity=1e-7,\n",
        "                 min_child_weight=1,\n",
        "                 sub_sample=0.8,\n",
        "                 sub_feature=0.8,\n",
        "                 reg_lambda=1,\n",
        "                 n_jobs=1,\n",
        "                 random_state=None):\n",
        "        super().__init__(n_estimators=n_estimators,\n",
        "                         max_depth=max_depth,\n",
        "                         learning_rate=learning_rate,\n",
        "                         min_samples_split=min_samples_split,\n",
        "                         min_impurity=min_impurity,\n",
        "                         min_child_weight=min_child_weight,\n",
        "                         sub_sample=sub_sample,\n",
        "                         sub_feature=sub_feature,\n",
        "                         reg_lambda=reg_lambda,\n",
        "                         n_jobs=n_jobs,\n",
        "                         random_state=random_state\n",
        "                         )\n",
        "        \n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y = to_categorical(y)\n",
        "        super().fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict_prob(self, X):\n",
        "        y_pred = super().predict(X)\n",
        "        # Turn into probability distribution (Softmax)\n",
        "        y_prob = np.exp(y_pred) / np.sum(np.exp(y_pred), axis=1, keepdims=True)\n",
        "\n",
        "        return y_prob\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_prob = self.predict_prob(X)\n",
        "        # Set label to the value that maximizes probability\n",
        "        y_pred = np.argmax(y_prob, axis=1)\n",
        "        return y_pred\n",
        "\n",
        "    def train(self, X, y,val_data,metrics='default'):\n",
        "        val_X,val_y=val_data\n",
        "        y=to_categorical(y)\n",
        "        val_y=to_categorical(val_y)\n",
        "        super().train(X,y,val_X,val_y)\n",
        "        return self\n"
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}