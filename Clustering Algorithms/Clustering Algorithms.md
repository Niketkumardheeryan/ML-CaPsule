## Clustering Algorithms

Clustering is an unsupervised machine learning task. Using a clustering algorithm means giving the algorithm a lot of unlabeled input and letting it find a group of data it can cluster on. It is often used as a data analysis technique to uncover interesting patterns. from the data.

These groups are called clusters. A cluster is a group of data points that are similar to each other based on their relationship to surrounding data points. Clustering is used for tasks such as feature engineering or pattern search.

### Types of clustering algorithms

There are different types of clustering algorithms that deal with all kinds of unique data.

- **Density-Based**
  Density-Based Clustering groups data into regions of high data point density surrounded by regions of low data point density. Basically, the algorithm finds places with many data points and calls those clusters.The most surprising thing about this is that clusters can be of any shape,not limited by expected conditions.This type of clustering algorithm is generally ignored because it does not assign outliers to the cluster.

- **Distribution-Based**
  When using the distribution-based clustering approach, all data points are considered part of a cluster based on their probability of belonging to that cluster.This works as follows: there is a centroid and as the distance of the data point from the centroid increases, the probability that it is part of this cluster decreases.

- **Centroid-based**
  Centroid-based clustering is probably the most popular one. It's a bit sensitive to the initial parameters provided, but it's fast and efficient.This type of algorithm separates data points based on multiple centroids in the data. Each data point is assigned to a cluster based on the squared distance from the centroid. This is the most commonly used type of clustering.

- **Hierarchical based**
  Hierarchical clustering is typically used for hierarchical data such as company or taxonomy databases. Build the cluster tree so that everything is organized from top to bottom.This is more limited than other types of clustering, but is ideal for certain types of data sets.

### Different Clustering Algorithms

The different clustering algorithms are as follows along with their implementation [here](./Clustering%20Algorithms/Clustering_Algorithms.ipynb)

- **K-means clustering algorithm**
  K-means clustering is the most commonly used clustering algorithm. It is a centroid-based algorithm and is the simplest unsupervised learning algorithm.This algorithm attempts to minimize the variance of data points within a cluster. K-means is best used for small data sets because it iterates over all data points. In other words, if there are a large number of data points in the data set, it will take longer to classify the data points.K-means does not scale well because it groups data points in this way.

- **DBSCAN clustering algorithm**
  DBSCAN stands for density-based spatial clustering of applications with noise.This is a density-based clustering algorithm as opposed to kmeans.This is a good algorithm for finding edges in a data set.

  - Find free-form clusters based on the density of data points in different regions.
  - Divide the region into low-density regions to detect outliers between high-density clusters.
    This algorithm is better than kmeans when dealing with oddly shaped data.
    DBSCAN uses two parameters to determine how the cluster is defined.
  - minPts (minimum number of data points that must be clustered together for an area to be considered dense) and
  - eps (areas with equal data points, other data points).

- **Gaussian Mixture Model algorithm**
  One problem with kmeans is that the data must be in a circular form. The way kmeans calculates distances between data points is due to circular paths, so non-circular data is grouped incorrectly.Gaussian mixture model solved the problem.Mixed Gaussian models fit arbitrary shape data using multiple Gaussian distributions.This hybrid model has several single Gaussian models that act as hidden layers. Thus, the model calculates the probability that a data point belongs to a particular Gaussian distribution, which will fall into that cluster.

- **BIRCH algorithm**
  The Balance Iterative Reducing and Clustering using Hierarchies (BIRCH) algorithm works better on large data sets than the k-means algorithm.It breaks the data into little summaries that are clustered instead of the original data points. The summaries hold as much distribution information about the data points as possible.This algorithm is commonly used with other clustering algorithm because the other clustering techniques can be used on the summaries generated by BIRCH.The main downside of the BIRCH algorithm is that it only works on numeric data values. It cannot be used for categorical values ​​unless some data transformation is done.

- **Affinity Propagation clustering algorithm**
  This clustering algorithm is completely different from other algorithms in the way it clusters data.Each data point is connected to all the other data points to tell you how similar they are to each other. Then clusters of data start to appear. You do not need to tell this algorithm how many clusters to expect in the initialization parameter.When messaging between data points, a data set called a sample appears to be a cluster. A pattern was found after data points communicated with each other and formed consensus on which data points best represent the cluster.It is a good algorithm to start with if you are not sure how many clusters to expect, for example in a computer vision problem.

- **Mean-Shift clustering algorithm**
  This is another algorithm that is particularly useful for handling images and computer vision processing.Mean-shift is similar to the BIRCH algorithm because it also finds clusters without an initial number of clusters being set.This is a hierarchical clustering algorithm, but the downside is that it doesn't scale well when working with large data sets.It works by iterating over all of the data points and shifts them towards the mode. The mode in this context is the high density area of data points in a region.That's why this algorithm is referred to as the mode-seeking algorithm. It will go through this iterative process with each data point and move them closer to where other data points are until all data points have been assigned to a cluster.

- **OPTICS algorithm**
  OPTICS stands for Ordering Points to Identify the Clustering Structure. A density-based algorithm similar to DBSCAN, but better because it can find meaningful clusters in data with varying densities. This is done by sorting the data points so that the nearest points are neighbors in order.This facilitates the detection of clusters with different densities. The OPTICS algorithm, similar to DBSCAN (although slower than DBSCAN), processes each data point only once. Also, for each data point, a special distance is stored to indicate that the point belongs to a particular cluster.

- **Agglomerative Hierarchy clustering algorithm**
  This is the most common type of hierarchical clustering algorithm. It's used to group objects in clusters based on how similar they are to each other.This is a form of bottomup clustering, where each data point is assigned to its own cluster. Then those clusters get joined together.At each iteration, similar clusters are merged until all of the data points are part of one big root cluster.Cohesive clustering is best for finding small clusters. The end result looks like a dendrogram, making it easy to visualize the clusters once the algorithm is done.

- **Divisive Hierarchical clustering algorithm**
  There is another hierarchical algorithm as opposed to a cohesive approach.

  - Start with a top-down clustering strategy.
  - Start with one large root cluster and break it down into individual clusters.
    Studies show that this produces more accurate hierarchies than cohesive clustering, but it is much more complex.

- **Mini-Batch K-means**
  This is similar to K-means, except that it uses small random chunks of data of a fixed size so they can be stored in memory. This helps it run faster than K-means so it converges to a solution in less time.The drawback to this algorithm is that the speed boost will cost you some cluster quality.

- **Spectral Clustering**
  This algorithm is completely different from the others .It works by taking advantage of graph theory. This algorithm doesn't make any initial guesses about the clusters that are in the data set. It treats data points like nodes in a graph and clusters are found based on communities of nodes that have connecting edges.

### Applications

- Clustering is especially useful for exploring data you know nothing about.
- Some real world applications of clustering include
  - Fraud detection in insurance
  - Categorizing books in a library
  - Customer segmentation in marketing.
- It can also be used in larger problems, like
  - Earthquake analysis
  - City planning.

### References

https://machinelearningmastery.com/clustering-algorithms-with-python/

#### CONTRIBUTED BY

[Shreya Ghosh](https://github.com/shreya024)
